---
dlayout:    post
title:      FlashLLM
subtitle:   Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity
date:       2023-12-08
author:     Kylin
header-img: img/flashLLM.jpg
catalog: true
tags:
    - paper
---



[TOC]

### Abs

**Challenge**: Unstructured model pruning 和 highly-structured tensor core hardware不适配

**Observation**：bottleneck of generative model inference is the several skinny matrix multiplications （decode阶段）。Decode stage would be significantly under-utilized due to low computational in- tensity

**insight**: 解决IO-bound，允许一部分冗余计算。address the signi￿cant memory bandwidth bottleneck while tolerating redundant computations that are not critical for end-to-end performance on tensor cores



### Intro & bg

这句话说挺好：There are three basic characteristics for practical model inference: **accuracy**, **efficiency** (i.e., latency and throughput), and **cost** (i.e., how much hardware resource it consumes). 

现有的model pruning方法存在两个问题：

- difficult to **support unstructured sparsity on modern GPU architectures** efficiently, especially since the unstructured sparse MatMul (SpMM) is hard to support on the high-performance but highly-structured tensor core architecture.
- 实现的不好。the state-of-the-art unstructured SpMM implementations (e.g. cuSPARSE [40], Sputnik [10]) can not even outperform the dense counterpart (cuBLAS [39]) until the model sparsity is higher than 98% and 86%, respectively.

Insight：an innovative approach to support unstructured sparsity on tensor cores by leveraging sparse memory load to improve the limited memory bandwidth while e￿ectively tolerating redundant tensor-core computation

Challenges：

- sparse data storage and extraction
- designing the MatMul computation pipeline



### Method





### Reference

[^1]: Xia, Haojun, et al. "Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity." *arXiv preprint arXiv:2309.10285* (2023).

