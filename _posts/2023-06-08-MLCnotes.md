---
dlayout:    post
title:      Notes for Machine Learning Compilation
subtitle:   机器学习编译Notes
date:       2023-6-08
author:     Kylin
header-img: img/compiler.jpg
catalog: true
tags:
    - machine learning
---



[TOC]

### Intro

#### What's MLC?

Machine learning compilation (MLC) is the process of transforming and optimizing machine learning execution from its development form to its deployment form.

<img src="https://kylinhub.oss-cn-shanghai.aliyuncs.com/dev-deploy-form.png" alt="../_images/dev-deploy-form.png" style="zoom:37%;" />

machine learning compilation still differs from traditional compilation in many ways.

For example, the deployment form can be a set of pre-defined library functions, and the ML compilation only translates the development forms onto calls into those libraries. 



#### Goal of MLC?

- **Integration and dependency minimization.** 不必要的库就不需要引入了
- **Leveraging hardware native acceleration.** 利用不同的硬件特性
- **Optimization in general.** 比如内存、执行效率、分布式等



#### Key Elements in MLC？

Computation Graph: Tensor, Tensor Function



#### Abstraction and Implementation

- Abstraction usually allocate in development, whereas Implementation is in deployment 

In practice, we usually say that the more specialized version is an implementation of higher-level abstraction。比如说，汇编是一个高层算法的实现。

**Key idea of this lesson: Most MLC process can be viewed as transformation among tensor functions (that can be represented with different abstractions)**



#### Four Categories of Abstractions

- Computational Graphs
- Tensor Programs
- Libraries and Runtime
- Hardware Primitives



### Tensor Program Abstraction

#### Primitive Tensor Function

A typical model execution involves several computation steps that transform tensors from input to the final prediction, and each unit step is called a primitive tensor function.

<img src="https://kylinhub.oss-cn-shanghai.aliyuncs.com/primitive_tensor_func.png" style="zoom:37%;" />

#### Primitive Function Transformation

One most common MLC process that many frameworks offer is to transform the implementations of primitive functions(or dispatch them in runtime) to more optimized ones based on the environment.

![屏幕截图 2023-06-08 210116](https://kylinhub.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-06-08%20210116.png)

Approaches for primitive function transformation:

- Remap to library calls: e.g. cuda, add => cudaAdd
- Fine-grained program transformation



#### Tensor Program Transformation

<img src="https://kylinhub.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-06-08%20225933.png" alt="屏幕截图 2023-06-08 225933" style="zoom:80%;" />

- Key Elements of a Tensor Program

<img src="https://kylinhub.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-06-08%20230044.png" alt="屏幕截图 2023-06-08 230044" style="zoom:90%;" />

- Example Transformation:

<img src="https://kylinhub.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-06-08%20230216.png" alt="屏幕截图 2023-06-08 230216" style="zoom:67%;" />

<img src="https://kylinhub.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-06-08%20230253.png" alt="屏幕截图 2023-06-08 230253" style="zoom:67%;" />

<img src="https://kylinhub.oss-cn-shanghai.aliyuncs.com/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-06-08%20230317.png" alt="屏幕截图 2023-06-08 230317" style="zoom:67%;" />





















