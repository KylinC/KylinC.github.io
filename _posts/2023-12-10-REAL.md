---
dlayout:    post
title:      Real Bottlenck of Transformer
subtitle:   Transformer真正的优化瓶颈在哪里？
date:       2023-12-10
author:     Kylin
header-img: img/bottleneck.jpg
catalog: true
tags:
    - paper
---



[TOC]



### Preface







### Reference

[^1]: FLAT-Attention v.s. FlashAttention https://hackmd.io/@felixkao/HkZaooPD3
[^ 2]: Kao, Sheng-Chun, et al. "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks." *Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2*. 2023.

[^ 3]: Williams, Samuel, Andrew Waterman, and David Patterson. "Roofline: an insightful visual performance model for multicore architectures." *Communications of the ACM* 52.4 (2009): 65-76.
[^ 4]: Yang, Guo, et al. "Dynamic Stashing Quantization for Efficient Transformer Training." *arXiv preprint arXiv:2303.05295* (2023).

