---
dlayout:    post
title:      Real Bottlenck of Transformer
subtitle:   Transformer真正的优化瓶颈在哪里？
date:       2023-12-10
author:     Kylin
header-img: img/bottleneck.jpg
catalog: true
tags:
    - paper
---



[TOC]



#### ***关键瓶颈

> 这个问题主要是看你从什么方面看

1）从roofline model角度上看：

- 如果是对于transformer，优化瓶颈在于att (flash & flat) 的两个matmuls。为什么呢？因为这部分是memory-bound的：

  - 这部分是activation-activation计算，存在内存(GPU内)切换
  - activation计算涉及softmax这样的操作

  表现就是，即使增大batchsize，也不会缓解。

- 如果是LLM，优化瓶颈是在W_qkv,W_o,FFN$\times$2这些位置 (flashllm)，为什么呢？因为decode阶段的存在，这部分都是瘦矩阵参与计算，用不了tensorcore的并行性

2）从长文本上看：

- 性能上看：主要是计算和储存二次方的问题
- **最近听到一种有趣的说法，说瓶颈是在softmax（即使flash att也不能缓解）**



> softmax 这个时间确实会变长是长序列的瓶颈, 因为现在self-attention主流大家都在都在用flash attention 来加速, softmax这个计算本身无法并行, 再加入每一个新的元素需要scale来防止益处, flash attention softmax走的是 Online softmax, 多切出一个并行维度来先算local softmax 再合并来实现对这一计算的并行, 但前段时间自己试着把 sequence维度拉长后用flash attention是会报错的, 太长序列合在一个算子搞会超出Nv卡本身硬件计算上的一些block size限制, 长序列想继续走flash attention自己肯定要做改进的

> 这种不需要通信的softmax平替 有前途吗 https://arxiv.org/abs/2402.10930

> Ring attention可以吗？我是觉得ring起来可以以通信换一些显存和并行数量，也许可以解决你遇到的问题，不过softmax每个新block只会更新全局max吧，我记得rescale不是被解决了吗
>
> ring的原理和flash一样，相当于吧一张卡当一个local，算完以后再规约
>
> ring att原始文章 https://arxiv.org/abs/2105.13120

> 不太理解，不管是ring 还是flash attr ，任何其他的推理优化也好，要么就是靠算力省显存，要么拿空间换显存……矩阵的计算量变少了吗？如果计算量没有指数级的变少，那根本节省不了任何成本啊。分布式的计算，只是把计算分摊给了其他显卡，还额外增加了IO。尽量用SRAM确实少了IO，但是计算量并没有大的变化吧……所以，这200万汉字真的是完全没有稀释操作？？？kimi 家有金矿？求大佬解惑

> Megatron-LM已经有对context parallel的支持了
>
> https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/context_parallel.html

> Uless切的是 hidden dim 维度，softmax 还是需要完整序列，两个并不冲突，现在长序列都会同时用，但softmax的瓶颈还是会在



### Reference

[^1]: FLAT-Attention v.s. FlashAttention https://hackmd.io/@felixkao/HkZaooPD3
[^ 2]: Kao, Sheng-Chun, et al. "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks." *Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2*. 2023.

[^ 3]: Williams, Samuel, Andrew Waterman, and David Patterson. "Roofline: an insightful visual performance model for multicore architectures." *Communications of the ACM* 52.4 (2009): 65-76.
[^ 4]: Yang, Guo, et al. "Dynamic Stashing Quantization for Efficient Transformer Training." *arXiv preprint arXiv:2303.05295* (2023).

