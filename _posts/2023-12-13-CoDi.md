---
dlayout:    post
title:      CoDi Any to Any Generation
subtitle:   CoDi 系列论文 Review
date:       2023-12-13
author:     Kylin
header-img: img/icl.jpg
catalog: true
tags:
    - paper
---



[TOC]



### Any-to-Any Generation via Composable Diffusion

> NeurlPS 23

#### Abs

- 支持模态自由组合

> CoDi is **free** on input modality combination, even if they are not present in the training data.

- Highly customizable and flexible, CoDi achieves **strong** joint-modality generation quality, and **outperforms** or is **on par with** the unimodal state-of-the-art for single-modality synthesis. 

> 与单模态生成相当甚至超过

#### Intro

现有的多模态模型使用场景受限（e.g., text-to-text, text-to-image, image-to-text），虽然可以进行multi-step或者multi-module的拼接，但是这样的过程 **(1)** cumbersome and slow、并且 **(2)** 在输出流上表现出不连续性（post-processing的多模态方案）。

如果要训一个这种any composable-to-any composable的模型，有两个challenge：**(1)** 组合modality产生巨量计算数据需求；**(2)** 满足多种模态交叉的数据实际上很少。 

CoDi的方案：

- we train a latent diffusion model (LDM) for each modality，其实就是把输入对齐在一个**shared feature space**，这个步骤保证了单模态的生成质量
- 为了保证输出simultaneous的特性，要对齐给每一个diffuser加一个cross-attention module和environment encoder V，这两个组件project the latent variable of different LDMs into a **shared latent space**   



### Method

<img src="https://kylinhub.oss-cn-shanghai.aliyuncs.com/image-20231214234823561.png" alt="image-20231214234823561" style="zoom:50%;" />





### Reference

[^1]: 
