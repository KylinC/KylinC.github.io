---
dlayout:    post
title:      阿里饿了么2025届算法面筋
subtitle:   interview to Ele
date:       2024-03-22
author:     Kylin
header-img: img/bg_NLPseg.jpg
catalog: true
tags:
    - job
---



[TOC]

### 一面

- lora初始化参数？为什么一半是0

> 参考[^1]

![截屏2024-03-25 16.17.25](http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2024-03-25%2016.17.25.png)

- prenorm / postnorm区别

前比较明确的结论是：同一设置之下，Pre Norm结构往往更容易训练，但最终效果通常不如Post Norm[^2]。(迁移性能，就是sft之后在下游的性能)

简单来说，就是prenorm有退化，在transfomer layers比较多的情况下，其等效于宽网络，造成退化。postnorm削弱了恒等分支（残差连接），因为postnorm是在residual之后norm的，所以训练任务更难，但是也更不容易收敛。



- 量化（感知训练、后训练）

有两类：

Quantization-Aware Training (QAT) ：

> 在训练阶段就使用

Post-Training Quantization (PTQ) ：

> W8A16, 需要N卡支持
>
> uniform methods (i.e., Round-to-Nearest) and non-uniform methods



- 稀疏化、剪枝

稀疏化：矩阵层面，token语义层面

pruning：

> 结构化剪枝：Deja Vu直接剪掉head，early stop
>
> 非结构化：sparsity



- 位置编码，相对位置编码的好处

> rope, bloom, t5

捕捉相对位置的语意，对不同长度适用



- 外推能力[^3]

预测的时候用到了没训练过的位置编码（不管绝对还是相对）；

预测的时候注意力机制所处理的token数量远超训练时的数量。

一旦我们在模型中有效地整合了相对位置信息，增加 LLM 上下文窗口的最直接方法就是通过位置插值 (position interpolation，PI) 进行微调。



- pretrain的方法

**掩码语言模型（Masked Language Model, MLM）**：如BERT模型所采用的，随机遮盖输入文本中的某些单词（例如，15%的单词），然后让模型预测这些遮盖的单词。这种方法帮助模型学习到词语的上下文相关表示。

**因果语言模型（Causal Language Modeling, CLM）**：如GPT系列模型所采用的，根据前面的单词预测下一个单词。这种方法帮助模型学习生成性文本和长期依赖关系。

**Teacher Forcing**

**Clip**

- 手撕：给定整数数组 nums 和整数 k，请返回数组中第 k 个最大的元素。

```python
nums = [2,5,3,1,4]

import random

def quickselect(nums, k):
    l,r = 0,len(nums)-1
    while l<r:
        i,j = l,r
        rd = random.randint(l,r)
        nums[l],nums[rd] = nums[rd],nums[l]
        pivot = nums[i]
        while i<j:
            while i<j and nums[j]>=pivot:
                j-=1
            nums[i]=nums[j]
            while i<j and nums[i]<=pivot:
                i+=1
            nums[j]=nums[i]
        nums[i]=pivot
        if k<i:
            r=i-1
        elif k>i:
            l=i+1
        else:
            break
        
k=4
quickselect(nums,k)
print(nums[k])
```

让我们来回顾一下quicksort

```python
nums = [2,5,3,1,4]

import random

def partition(nums,l,r):
    rd = random.randint(l,r)
    nums[l],nums[rd] = nums[rd],nums[l]
    pivot = nums[l]
    while l<r:
        while l<r and nums[r]>=pivot:
            r-=1
        nums[l] = nums[r]
        while l<r and nums[l]<=pivot:
            l+=1
        nums[r] = nums[l]
    nums[l] = pivot
    return l

def quicksort(nums,l,r):
    if r-l<=1:
        return 
    mi = partition(nums,l,r-1)
    quicksort(nums,l,mi)
    quicksort(nums,mi+1,r)
    
        
quicksort(nums,0,len(nums))
print(nums)
```



### 二面

说rlhf的细节？loss？

scaling law相关的论文看过吗？

训练相关的优化策略了解吗？

问项目







### Reference

[^1]: 简单理解大模型参数高效微调中的LoRA(Low-Rank Adaptation) https://blog.csdn.net/qq_40714949/article/details/131988734
[^2]: 为什么Pre Norm的效果不如Post Norm？https://kexue.fm/archives/9009
[^3]: LLM（廿三）：LLM 中的长文本问题. https://zhuanlan.zhihu.com/p/640641794
