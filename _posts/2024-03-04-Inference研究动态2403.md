---
dlayout:    post
title:      LLM Inference Optimization 2403 Review
subtitle:   LLM优化技术进展
date:       2024-03-04
author:     Kylin
header-img: img/LLMInferReview2403.jpg
catalog: true
tags:
    - paper
---



[TOC]

### Cascade Inference[^1]

> 带宽高效的共享前缀自注意力操作



###  SGLang[^3]

> 提出了RadixAttention，其中KV-Cache被组织成前缀树的形态，这类注意力操作可以使用多级的Cascade Inference加速



### LMaaS[^4]



### XXX







### Reference

[^1]: Cascade Inference: Memory Bandwidth Efficient Shared Prefix Batch Decoding. https://flashinfer.ai/2024/02/02/cascade-inference.html

[^2]: 带宽高效的共享前缀自注意力操作. https://zhuanlan.zhihu.com/p/681514497
[^3]: Efficiently Programming Large Language Models using SGLang. https://arxiv.org/abs/2312.07104

[^4]: A Survey on Effective Invocation Methods of Massive LLM Services















