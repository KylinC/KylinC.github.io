---
dlayout:    post
title:      面向算法岗学习八股
subtitle:   Job-Oriented Learning
date:       2023-3-29
author:     Kylin
header-img: img/post-cache.jpg
catalog: true
tags:
    - Job-Oriented Learning
---



[TOC]



#### Norm

- L1范数（稀疏规则算子, Lasso regularization）: 为x向量各个元素绝对值之和
- L2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或Frobenius范数
- Lp范数: 为x向量各个元素绝对值p次方和的1/p次方

> Lasso regularization是一种用于线性回归模型的正则化方法，它通过在模型的损失函数中添加L1范数惩罚项来惩罚模型中的大型权重，从而避免过拟合。Lasso regularization的数学表达式为：
>
> 
>
> $$ L_{\text{lasso}}(\beta) = \frac{1}{2n} \lVert \mathbf{y} - \mathbf{X}\beta \rVert_2^2 + \alpha \lVert \beta \rVert_1 $$
>
> 
>
> 其中，$\mathbf{y}$是目标变量向量，$\mathbf{X}$是特征矩阵，$\beta$是模型的权重向量，$n$是样本数量，$\alpha$是正则化强度超参数。L1范数惩罚项$\lVert \beta \rVert_1$可以促使模型中的某些权重变为0，从而实现特征选择的效果。
>
> 
>
> 而Lasso loss是指Lasso回归模型的损失函数，它是由数据集中的观测值与Lasso regularization的总和构成的。Lasso loss的数学表达式为：
>
> 
>
> $$ L_{\text{lasso}}(\beta) = \frac{1}{2n} \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2 + \alpha \sum_{j=1}^p \lvert \beta_j \rvert $$
>
> 
>
> 其中，$y_i$是第$i$个观测值的目标变量，$x_{ij}$是第$i$个观测值的第$j$个特征，$\beta_0$是模型的截距，$\beta_j$是模型的第$j$个权重，$n$是样本数量，$p$是特征数量，$\alpha$是正则化强度超参数。L1范数惩罚项$\sum_{j=1}^p \lvert \beta_j \rvert$可以促使模型中的某些权重变为0，从而实现特征选择的效果。

- 对比：
  - L1范数可以使权值稀疏，方便特征提取。（使得不重要的特征权重为0，反之不为0）https://lcqbit11.blog.csdn.net/article/details/103101607
  - L2范数可以防止过拟合，提升模型的泛化能力。
  - L1倾向选择更少特征，使得其他特征权重为0；L2选择更多特征，使得权重接近于0。



#### Overfitting 产生原因

- 数据量太小 or 噪声太大
- trainset和testset数据分布不一致
- 模型复杂度太大
- 过度训练

- 解决方法：
  - 正则化（正则化可以限制模型的复杂度） or dropout
  - 交叉验证 or 创建一个验证集
  - 特征选择/特征降维
  - 数据集扩增
  - 降低模型复杂度
  - batch normalizatin



#### Decision Tree

- ID3 算法的核心是在决策树的每个节点上应用信息增益准则选择特征，递归地构架决策树。
- C4.5 算法的核心是在生成过程中用信息增益比来选择特征。
- CART 树算法的核心是在生成过程用基尼指数来选择特征。

基于决策树的算法有随机森林、GBDT、Xgboost 等。



#### 梯度问题

1.梯度消失：
根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0。

可以采用ReLU激活函数有效的解决梯度消失的情况。（ReLU函数在输入值大于0时，导数恒为1，这意味着反向传播时梯度不会消失。）

2.梯度膨胀：
根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大。

可以通过激活函数来解决。

3.梯度爆炸：
针对梯度爆炸问题，解决方案是引入Gradient Clipping(梯度裁剪)。

通过Gradient Clipping，将梯度约束在一个范围内，这样不会使得梯度过大。



