---
dlayout:    post
title:      FLAT Attention
subtitle:   An Optimized Dataflow for Mitigating Attention Bottlenecks
date:       2023-11-09
author:     Kylin
header-img: img/bg-owncloud.jpg
catalog: true
tags:
    - paper
---



[TOC]

### Abs

Attention的问题：large memory requirements and computational complexity。=> This limitation is due to inherently limited data reuse opportunities and quadratic growth in memory footprints

我们的解决方案：a tailored dataflow optimization => transforming the memory footprint quadratic growth to merely a linear one

Our method both mitigates the off- chip bandwidth bottleneck as well as reduces the on-chip memory requirement.



### Reference

[^1]: FLAT-Attention v.s. FlashAttention https://hackmd.io/@felixkao/HkZaooPD3
