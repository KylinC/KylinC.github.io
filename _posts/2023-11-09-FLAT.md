---
dlayout:    post
title:      FLAT Attention
subtitle:   An Optimized Dataflow for Mitigating Attention Bottlenecks
date:       2023-11-09
author:     Kylin
header-img: img/dataflow.jpg
catalog: true
tags:
    - paper
---



[TOC]

### Abs

Attention的问题：large memory requirements and computational complexity => This limitation is due to inherently **limited data reuse** opportunities and **quadratic growth in memory footprints**

我们的解决方案：a tailored dataflow optimization => transforming the memory footprint quadratic growth to merely a linear one

Our method both mitigates the off- chip bandwidth bottleneck as well as reduces the on-chip memory requirement.

所以解决的问题（off-chip data transfer）、解决的方式（tiling & scheduling）和flash attention是一致的。



### Intro & bg

相比于Flash Attention的故事一直在紧抓SRAM-HBM访存次数问题，Flat Attention关注operational intensity（当然其实两者在本质上也是一致的）

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/20231208174117.png" alt="截屏2023-12-08 17.40.55" style="zoom:40%;" />

Flat把Attention中的op分为两类，一类是Activation-Weight operators (Q/K/V/O)，一类是Activation-Activation operators (L/A)，然后就会发现 L/A 限制了整体的operational intensity，即使增大batch size也没有用。

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/20231208172859.png" alt="截屏2023-12-08 17.26.58" style="zoom:40%;" />



### Method[^1]

#### Tiling Strategy Comparisons

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/20231208175247.png" alt="HyirTjABh" style="zoom:50%;" />

*The tiling strategy difference between FLAT-Attention and FlashAttention. FlashAttention uses block tiling and weight stationary. FLAT-Attention uses row tiling (row-granularity) and output stationary.*

#### Scheduling Strategy (Dataflow) Comparisons

##### Flash Attention

![S1geyhArn](http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/20231208175521.png)

![H1Fxy3AH3](http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/20231208175529.png)

##### FLAT attention

![BkXMy2RBn](http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/20231208175837.png)

![SyRSk20rh](http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/20231208175845.png)



### Qualtively Comparisons[^1]（FLAT vs. Flash）

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/20231208175148.png" alt="rkQU5jRHh" style="zoom:40%;" />



### Reference

[^1]: FLAT-Attention v.s. FlashAttention https://hackmd.io/@felixkao/HkZaooPD3
[^ 2]: Kao, Sheng-Chun, et al. "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks." *Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2*. 2023.

