---
dlayout:    post
title:      KV Cache
subtitle:   KV Cache 关键的优化技术
date:       2023-8-21
author:     Kylin
header-img: img/VIT.jpg
catalog: true
tags:
    - nlp
---



[TOC]



> 一个绝好的教程：https://www.youtube.com/watch?v=80bIUggRJf4
>
> KV-cache优化技术总结：https://zhuanlan.zhihu.com/p/659770503



### KV-cache介绍

> KV-cache本身就是model.generate baseline式的优化方法

- Attention的公式：

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-08-21%2016.23.18.png" alt="截屏2023-08-21 16.23.18" style="zoom:37%;" />



- Q是对于当前token的编码，但是K、V是对于之前+当前token的编码矩阵

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-08-21%2016.23.51.png" alt="截屏2023-08-21 16.23.51" style="zoom:33%;" />



- 在Transformer架构中，只有self attention会进行token间交互，这里就会使用KV-cache：**即记录下之前的kv矩阵**

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-08-21%2016.22.48.png" alt="截屏2023-08-21 16.22.48" style="zoom:33%;" />

> 一般情况下，kv-cache常驻显存





### KV-cache显存计算

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-08-21%2016.26.29.png" alt="截屏2023-08-21 16.26.29" style="zoom:40%;" />

- 例子：OPT-30B

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F20213-08-21%2016.28.08.png" alt="截屏20213-08-21 16.28.08" style="zoom:47%;" />

- 例子：llama-7B

我计算了一下，在256 token的时候需要0.125G



### KV-cache优化技术

> 进一步的，我们怎么优化KV-cache?



#### 公用KV-cache：MQA，GQA

MQA (Multi Query Attention，多查询注意力) 是多头注意力的一种变体。其主要区别在于，**在 MQA 中不同的注意力头共享一个K和V的集合，每个头只单独保留了一份查询参数**。因此K和V的矩阵仅有一份，这大幅度减少了显存占用，使其更高效。由于MQA改变了注意力机制的结构，因此模型通常需要从训练开始就支持 MQA 。也可以通过对已经训练好的模型进行微调来添加多查询注意力支持，仅需要约 5% 的原始训练数据量 就可以达到不错的效果。包括 Falcon、SantaCoder、StarCoder 等在内很多模型都采用了 MQA 机制。

GQA（Grouped Query Attention）保留了一定数量的KV-cache，使得其在MHA和MQA之间平衡。

![image-20231010231326213](https://kylinhub.oss-cn-shanghai.aliyuncs.com/image-20231010231326213.png)



#### 窗口优化

KV cache 的作用是计算 attention，当推理时的文本长度T大于训练时的最大长度 L 时，一个自然的想法就是滑动窗口。这里又有两种方式：

1）固定窗口长度（图 b），典型代表即 [Longformer](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2004.05150v2.pdf)，该方法实现简单，而且空间复杂度只有O(TL) ，但是精度下降比较大；

2）KV 重计算（图 c），该方法需要每次计算都重新计算长度为L的 KV-cache，由于重计算的存在，其精度可以保证，但是性能损失比较大；

![image-20231010232527341](https://kylinhub.oss-cn-shanghai.aliyuncs.com/image-20231010232527341.png)



#### 稀疏&量化

[lmdeploy](https://link.zhihu.com/?target=https%3A//github.com/InternLM/lmdeploy): KV-cache的量化

[H2O](https://link.zhihu.com/?target=https%3A//browse.arxiv.org/pdf/2306.14048.pdf): 通过动态的评价方式来判断需要保留和废弃的KV值



#### 储存优化

例如 vLLM 的 PagedAttention



### StreamingLLM

StreamingLLM 的基本思想同样是来源于窗口优化思想，其最大的创新在于提出了识别并保存模型固有的「注意力池」（attention sinks）锚定其推理的初始 token。

核心的发现：[Lost in the Middle](https://link.zhihu.com/?target=https%3A//browse.arxiv.org/pdf/2307.03172.pdf) ，即注意力集中在头尾：

![image-20231010233859779](https://kylinhub.oss-cn-shanghai.aliyuncs.com/image-20231010233859779.png)

StreamingLLM 的做法可以理解为将滑窗 L 分为固定部分加上滑动部分，即： [4,L−4] ，那么需要 GPU 的显存的上限就是确定的，即可保证长度的“无限”增加。



