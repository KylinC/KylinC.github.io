---
dlayout:    post
title:      KV Cache
subtitle:   KV Cache model.generate 关键的优化技术
date:       2023-8-21
author:     Kylin
header-img: img/VIT.jpg
catalog: true
tags:
    - nlp
---



[TOC]



> 一个绝好的教程：https://www.youtube.com/watch?v=80bIUggRJf4



### KV-cache介绍

- Attention的公式：

![截屏2023-08-21 16.23.18](http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-08-21%2016.23.18.png)



- Q是对于当前token的编码，但是K、V是对于之前+当前token的编码矩阵

![截屏2023-08-21 16.23.51](http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-08-21%2016.23.51.png)



- 在Transformer架构中，只有self attention会进行token间交互，这里就会使用KV-cache：**即记录下之前的kv矩阵**

![截屏2023-08-21 16.22.48](http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-08-21%2016.22.48.png)

> 一般情况下，kv-cache常驻显存



### KV-cache显存计算

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-08-21%2016.26.29.png" alt="截屏2023-08-21 16.26.29" style="zoom:40%;" />

- 例子：OPT-30B

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F20213-08-21%2016.28.08.png" alt="截屏20213-08-21 16.28.08" style="zoom:47%;" />

- 例子：llama-7B

我计算了一下，在256 token的时候需要0.125G







