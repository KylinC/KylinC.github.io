---
dlayout:    post
title:      novel ideas for MLLM research
subtitle:   comprehensive survey for MLLM research
date:       2023-6-25
author:     Kylin
header-img: img/compiler.jpg
catalog: true
tags:
    - paper
---



[TOC]



idea：

- rethinking 传统的 image caption 的问题（从前研究没法解决的LLM可以解决的）
- caption组合泛化外部知识结合
- 以人为中心的 Video Caption
- Dense Video Caption 的动态
- 语言多样性的 Video Caption
- 细粒度的 Caption
- 新闻文本的caption/短视频评论的caption



##### Image Caption

Deep Visual-Semantic Alignments... , Li Fei-Fei 视觉语意对齐 2014

图像信息（高维度的）远大于文本信息

show and tell 2015

show, attend and tell 2016 (有colab)

- 新的paper

GIT: A Generative Image-to-text Transformer for Vision and Language

BLIP2：目前冻结式的两阶段训练；zero-shot效果极好（MoE的替代方案）

ClipCap：

> prefix tuning："Prefix-tuning"是一种用于微调大型预训练语言模型的技术，如OpenAI的GPT-3或GPT-4。这种技术的基本思想是在模型的输入序列的开头添加一个可训练的、固定长度的"前缀"，并只在这个前缀上进行参数更新，而保持原模型的其余参数不变。在这种设置下，前缀实际上可以看作是一个可调整的任务特定嵌入，它可以帮助指导模型在特定任务上的行为。
>
> P-tuning："P-tuning" 是 "Pattern-tuning" 的简称，是一种新颖的微调技术，用于训练和优化大型预训练语言模型，如 OpenAI 的 GPT-3 或 GPT-4。这种方法的关键思想是将特定的任务表示为填充（或完形填空）问题，并且构建一个模式或模板（即"Pattern"），然后在训练过程中找到最佳的参数。举个例子，假设你有一个情感分析任务，你可以创建一个模式如 "The sentiment of the sentence '___' is ___." ，然后模型的任务就是填充这个模式，提供正确的情感输出。
>
> Prompt：在自然语言处理（NLP）和人工智能（AI）领域，"Prompt"通常是指引导或触发模型生成特定输出的输入文本。对于一种称为"生成模型"的模型，prompt通常作为模型的输入，并启动生成过程。
>
> fine-tuning：就是小规模训练呗



##### Video Caption

Idea:

中文video caption

image attention迁移

zero-shotvideo caption

前沿：

End to end dense video caption parallel （代码好跑）

global object propasals for improving 

看下VAE DALL-E



##### LLMs pretrain

A survey of LLMs:

主要研究集中在4个方面：pre-train、domain fine-tuning、利用、能力评估



idea: 



填平语义鸿沟：**UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning**

正向例子和反向例子：回译 vs. 检索

**vlbert**：简单用image token代替bert token；how nlp to cv

nlpcc idea: 通过偏旁构造vector













