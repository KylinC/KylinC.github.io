---
dlayout:    post
title:      All About Deformable DETR
subtitle:   Deformable Transformers for end-to-end object detection 论文讲解
date:       2023-4-29
author:     Kylin
header-img: img/DETR.jpg
catalog: true
tags:
    - Object Detection
---



[TOC]

### Background

DETR是一个简洁的pipeline，但几乎不可用 DETR提出了一套不同于Dense Prediction的pipeline，将检测视为一个Set Prediction问题，成功去掉了Anchor Generation和NMS。 

#### Problem

计算量问题：但是在实际使用中，DETR在训练阶段面临难以收敛的困难，正常模型最多需要36个ep收敛，DETR需要500个ep，一个完整的训练需要8卡v100训练一周，开销过高。 在测试阶段，Transformer因为计算量的问题，只能在分辨率最低的Feature Map上运行，这导致小目标上的性能很差。 

#### Contribution

提出了一种改进的Attention (?不知道算不算) 机制，收敛速度快，精度高。

#### Research Meaning

说明了DETR收敛慢的原因 

引发了对Attention机制的一系列思考 





### Paper：Rethinking DETR

**DETR**的缺点

1. DETR收敛慢。
2. DETR小目标性能差。

**造成这些缺点的原因**

1. Attention Map变稀疏需要很长时间 
2. Transformer计算量大，只能运行在最 后一层feature map上，这会导致小物体性能差。



#### 稀疏度收敛问题

**Paper**: Rethinking Transformer-based Set Predition for Object Detection

稀疏度的计算（负熵）:
$$
\frac{1}{m} \sum_{j=1}^m P\left(a_{i, j}\right) \log P\left(a_{i, j}\right)
$$
矩阵中的0和1越多，这个数值越接近0.

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-29%2020.31.15.png" alt="截屏2023-04-29 20.31.15" style="zoom:50%;" />

论文还猜想，匈牙利匹配的不稳定也可能是需要很长训练时长的原因。但是实验否定了。

**论文结论是decoder造成了收敛慢。**



#### 小物体识别问题

下采样次数过多后，小物体会看不见，比较成熟的方案是用FPN， 用高分辨率的feature map预测小物体。

但是，Transformer计算开销过大。 高分辨率下面临计算资源不足的问题。 



#### Solution

> 去掉 decoder 

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-29%2020.40.10.png" alt="截屏2023-04-29 20.40.10" style="zoom:40%;" />



### Deformable DETR

#### Architecture

1. 主体结构和DETR一致.
2. 利用了多层feature map 
3. 所有的 Attention 都是 Deformable Attention 这个词。

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-29%2020.48.49.png" alt="截屏2023-04-29 20.48.49" style="zoom:50%;" />

> Deformable是什么？
>
> Paper: Deformable Convolutional Networks
>
> - Deformable指可变形
> - 与标准的方形卷积相对应
> - 参与卷积计算的点是可变的
>
> <img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-29%2020.53.24.png" alt="截屏2023-04-29 20.53.24" style="zoom:43%;" />
>
> Motivation: 希望Conv的感受野均来自目标instance上，而不包含背景
>
> <img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-29%2020.59.06-1.png" alt="截屏2023-04-29 20.59.06-1" style="zoom:50%;" />
>
> Implement：预测一个offset，之所以是2N的channel：一个N是x偏置、一个N是y偏置
>
> <img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-29%2021.01.36.png" alt="截屏2023-04-29 21.01.36" style="zoom:50%;" />



#### Deformable Attention

##### 公式表述

- Attention的另一种写法：

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-29%2021.18.22.png" alt="截屏2023-04-29 21.18.22" style="zoom:38%;" />

- Deformable Attention的写法：

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-29%2021.19.58.png" alt="截屏2023-04-29 21.19.58" style="zoom:35%;" />

##### Architecture

![截屏2023-04-29 21.24.42](http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-29%2021.24.42.png)

1. Sample数量K提前定好 
2. V是通过offset找到的 
3. offset是linear(q)得到的 
4. 这里没有k 
5. Attention Map也是 linear(q) 之后过一个softmax得到的。 

> 为什么 attention 是 Linear(q) 得到的？
>
> Paper: Synthesizer: Rethinking Self-Attention for Transformer Models 
>
> <img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-30%2010.22.35.png" alt="截屏2023-04-30 10.22.35" style="zoom:47%;" />
>
> 最后结果证明Linear和Random效果也是可以的

##### MS Deformable Attention

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-30%2010.27.09.png" alt="截屏2023-04-30 10.27.09" style="zoom:50%;" />



1. MSDeformAttn代替了 Encoder中的MSA 
2. MSDeformAttn代替了 Decoder中的Cross-MSA  
3. 保留了DETR Decoder中 的MSA  



#### Deformable DETR Encoder

1. 经过一个没有Top-down的FPN后得到4个feature map作为encoder输入 
2. Q为Feature Map上的一个点，为区分哪一层还多加了一个embeding 

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-30%2016.17.59.png" alt="截屏2023-04-30 16.17.59" style="zoom:50%;" />



#### Deformable DETR Decoder

1. Decoder的输入是Encoder的输出

2. 和DETR一样，Q是object query

3. Object query不在feature map上，公式里的 $p_q$ 怎么得到?

   <img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-30%2016.20.30.png" alt="截屏2023-04-30 16.20.30" style="zoom:50%;" />

4. 通过一个linear

5. 还有一点不同，这里预测的box是相对于pq的偏移。



#### Iterative Bbox refinement

1. 简单说就是把上一个decoder预测的bbox当作本个decoder的输入
2. 有一点类似于Cascade RCNN

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-30%2016.23.58.png" alt="截屏2023-04-30 16.23.58" style="zoom:50%;" />



#### Two-Stage Deformable DETR

1. 把Encoder中的每一个pixel当作一个object query。 
2. 预测出结果后，选Top score个当作decoder的object query输入。
3.  有一点类似于Faster RCNN，这里encoder当作rpn，decoder当作rcnn 

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-30%2016.25.10.png" alt="截屏2023-04-30 16.25.10" style="zoom:50%;" />



### Experiments

主要的设置follow了DETR，另外M=8，K=4（sampling points for each attention）

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-30%2016.29.52.png" alt="截屏2023-04-30 16.29.52" style="zoom:50%;" />

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-30%2016.30.18.png" alt="截屏2023-04-30 16.30.18" style="zoom:50%;" />



### Ablation Studies

Attention及FPN的作用: 

<img src="http://kylinhub.oss-cn-shanghai.aliyuncs.com/uPic/%E6%88%AA%E5%B1%8F2023-04-30%2016.53.13.png" alt="截屏2023-04-30 16.53.13" style="zoom:50%;" />



### Conclusion

你觉得这还是Attention吗? 在Deformable Attn上有没有什么创新的思路?

这属于见仁见智的问题，区别与联系建议参考: Paper: An Empirical Study of Spatial Attention Mechanisms in Deep Networks

改进空间很小，这篇论文完成度很高。
