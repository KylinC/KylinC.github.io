---
dlayout:    post
title:      阿里高德2025届算法面筋
subtitle:   interview to Gaode
date:       2024-03-22
author:     Kylin
header-img: img/bg_NLPseg.jpg
catalog: true
tags:
    - job
---



[TOC]

### 一面

- clip细节：数据怎么构造、怎么训练、怎么设计loss

> 参考[^1]

- 怎么提高zero-shot的能力

对比学习、多任务学习

instruction tuning

- 我对多模态有什么了解

- BN层的作用

输入数据进行归一化，加速收敛

稳定输出，跳出鞍点

- BN层的缺点

对于文本数据，不同有效长度问题；**测试集上两个数据均值和方差差别很大**就不合适了 

附：LN 是对一个样本的一个时间步上的数据进行减均除标准差，然后再回放（参数学习）对应到普通线性回归就是一层节点求均除标准差。

- batchsize太大或者太小大的影响

batchsize 太小,会造成波动大；

batchsize 太大，系统层面上硬件要求高、训练速度慢；并且会陷入局部最优，没有泛化能力。

- 梯度消失、梯度爆炸是为啥？怎么解决

梯度消失是因为网络过深、用了sigmoid或者tanh等饱和激活函数，初始化问题

梯度爆炸是因为用了非饱和激活函数，初始化问题

**梯度消失**：

1. **使用ReLU激活函数**：ReLU（Rectified Linear Unit）及其变体（如Leaky ReLU、PReLU）在正区间的导数是常数，这可以帮助减轻梯度消失问题。
2. **使用残差连接**：残差网络（ResNet）通过引入残差连接来允许梯度直接流过网络的不同层，从而缓解梯度消失。
3. **适当的参数初始化**：使用He初始化或Xavier初始化等技术可以帮助缓解梯度消失，通过在初始化时考虑前一层的尺寸来保持激活函数的方差。

**梯度爆炸**：

1. **梯度裁剪**：通过设置阈值来限制梯度的最大值，确保梯度不会变得过大。
2. **权重正则化**：L1或L2正则化可以限制权重大小，间接帮助控制梯度的规模。
3. **改进的优化算法**：使用Adam、RMSprop等自适应学习率优化器可以帮助缓解梯度爆炸问题，因为它们会根据梯度的历史值调整每个参数的学习率。

对于**两者**：

**使用批归一化（Batch Normalization）**：批归一化可以稳定激活函数的输出，减少梯度消失和爆炸的可能性。

**层归一化（Layer Normalization）**：类似于批归一化，但在单个样本的所有激活上计算归一化，适合循环网络和Transformer。

- 正负样本差异大怎么解决

**重采样技术**

- **过采样少数类**：增加少数类样本的数量，使其与多数类样本的数量相当。这可以通过简单复制少数类样本或使用更复杂的方法如SMOTE（Synthetic Minority Over-sampling Technique）来合成新的少数类样本。
- **欠采样多数类**：减少多数类样本的数量以匹配少数类样本的数量。这种方法简单但可能会导致信息损失。

**改变损失函数**

- **使用加权损失**：对不同类别的样本赋予不同的权重。少数类样本可以被赋予更高的权重，从而在损失函数中占据更大的比重，促使模型更关注于少数类。
- **使用专门针对不平衡数据设计的损失函数**，如focal loss，这种损失函数设计用来减少易分类样本的权重，增加难分类样本的影响力。

**使用数据增强**

对少数类样本应用数据增强技术，生成更多的变体，从而增加其在数据集中的比例。对于图像数据，可以通过旋转、缩放、裁剪等操作生成新的样本；对于文本数据，可以采用同义词替换、句子重构等方法。

**选择合适的评价指标**

在正负样本差异大的情况下，准确率可能不是一个好的评价指标，因为即使模型总是预测多数类，准确率也可能很高。改用其他指标，如精确率、召回率、F1分数或ROC-AUC分数，这些指标能更好地反映模型对少数类的识别能力。

- GMM聚类

- mllm的loss怎么设计

其实还是cross entropy

- lora的参数有哪些

r，alpha，dropout

手撕：两个有序数组找中位数

```python
class Solution:
    def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -> float:
        def find(k):
            index1,index2 = 0,0
            while True:
                if index1==m:
                    return nums2[index2+k-1]
                if index2==n:
                    return nums1[index1+k-1]
                if k==1:
                    return min(nums1[index1],nums2[index2])
                newIndex1 = min(m-1,index1+k//2-1)
                newIndex2 = min(n-1,index2+k//2-1)
                p1,p2 = nums1[newIndex1],nums2[newIndex2]
                if p1<=p2:
                    k-=newIndex1-index1+1
                    index1=newIndex1+1
                else:
                    k-=newIndex2-index2+1
                    index2=newIndex2+1

        m, n = len(nums1),len(nums2)
        k = m+n
        if k%2:
            return find(k//2+1)
        else:
            return (find(k//2)+find(k//2+1))/2
```



### 二面

你之前项目（MLLM）里面大模型会有幻觉吗？一本正经说不存在的事情？

多模态大模型为什么会出现幻觉？是因为训练数据嘛？

幻觉解决方案？

lora可否a初始化为0，b随机初始化？

bn的作用？从5层神经网络上去理解？梯度爆炸是为啥？

lora微调和prompt engineering有什么异同？

手撕：给定一个只包含0和1的列表，其首尾相连，现在可任意交换两个元素的值，最少交换多少次所有的1能连在一起
输入：nums = [0,1,0,1,1,0,0]
输出：1

```python
nums = [0,1,0,1,1,0,0]

count = 0
res = 0
w = 0
for num in nums:
  if num==1:
    w+=1

nums = nums+nums
for i in range(0,w):
  if nums[i]==0:
    count+=1
res = count
for right in range(w,len(nums)):
  if nums[right]==0:
    count+=1
  if nums[right-w]==0:
    count-=1
  res = min(res,count)
print(res)
```





### Reference

[^1]: CLIP 核心代码解读. https://zhuanlan.zhihu.com/p/643033091
