---
dlayout:    post
title:      Scaling Autoregressive Multi-Modal Models
subtitle:   Pretraining and Instruction Tuning
date:       2023-12-02
author:     Kylin
header-img: img/bg-owncloud.jpg
catalog: true
tags:
    - paper
---



[TOC]

### Abs

It is the first **multi-modal model trained with a recipe adapted from text-only language models**  

统一的decode方法：self-contained contrastive decoding methods  

两个stage：

- large-scale retrieval-augmented pretraining stage   
- multi-task supervised fine-tuning (SFT) stage  



### Pretraining

加了一个 Retrieval Augmentation  



### Method





